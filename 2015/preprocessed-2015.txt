scala> val fivedf = spark.sql("select * from caps_proj.HWR2015")
fivedf: org.apache.spark.sql.DataFrame = [country: string, region: string ... 10 more fields]

scala> fivedf.show
+--------------------+--------------------+-----+------+-------------+-------+-------+-------+-------+-------+----------+-------+
|             country|              region|hrank|hscore|standarderror|economy| family| health|freedom|  trust|generosity|     dr|
+--------------------+--------------------+-----+------+-------------+-------+-------+-------+-------+-------+----------+-------+
|         Switzerland|      Western Europe|    1| 7.587|      0.03411|1.39651|1.34951|0.94143|0.66557|0.41978|   0.29678|2.51738|
|             Iceland|      Western Europe|    2| 7.561|      0.04884|1.30232|1.40223|0.94784|0.62877|0.14145|    0.4363|2.70201|
|             Denmark|      Western Europe|    3| 7.527|      0.03328|1.32548|1.36058|0.87464|0.64938|0.48357|   0.34139|2.49204|
|              Norway|      Western Europe|    4| 7.522|       0.0388|  1.459|1.33095|0.88521|0.66973|0.36503|   0.34699|2.46531|
|              Canada|       North America|    5| 7.427|      0.03553|1.32629|1.32261|0.90563|0.63297|0.32957|   0.45811|2.45176|
|             Finland|      Western Europe|    6| 7.406|       0.0314|1.29025|1.31826|0.88911|0.64169|0.41372|   0.23351|2.61955|
|         Netherlands|      Western Europe|    7| 7.378|      0.02799|1.32944|1.28017|0.89284|0.61576|0.31814|    0.4761| 2.4657|
|              Sweden|      Western Europe|    8| 7.364|      0.03157|1.33171|1.28907|0.91087| 0.6598|0.43844|   0.36262|2.37119|
|         New Zealand|Australia and New...|    9| 7.286|      0.03371|1.25018|1.31967|0.90837|0.63938|0.42922|   0.47501|2.26425|
|           Australia|Australia and New...|   10| 7.284|      0.04083|1.33358|1.30923|0.93156|0.65124|0.35637|   0.43562|2.26646|
|              Israel|Middle East and N...|   11| 7.278|       0.0347|1.22857|1.22393|0.91387|0.41319|0.07785|   0.33172|3.08854|
|          Costa Rica|Latin America and...|   12| 7.226|      0.04454|0.95578|1.23788|0.86027|0.63376|0.10583|   0.25497|3.17728|
|             Austria|      Western Europe|   13|   7.2|      0.03751|1.33723|1.29704|0.89042|0.62433|0.18676|   0.33088| 2.5332|
|              Mexico|Latin America and...|   14| 7.187|      0.04176|1.02054|0.91451|0.81444|0.48181|0.21312|   0.14074|3.60214|
|       United States|       North America|   15| 7.119|      0.03839|1.39451|1.24711|0.86179|0.54604| 0.1589|   0.40105|2.51011|
|              Brazil|Latin America and...|   16| 6.983|      0.04076|0.98124|1.23287|0.69702|0.49049|0.17521|   0.14574|3.26001|
|          Luxembourg|      Western Europe|   17| 6.946|      0.03499|1.56391|1.21963|0.91894|0.61583|0.37798|   0.28034|1.96961|
|             Ireland|      Western Europe|   18|  6.94|      0.03676|1.33596|1.36948|0.89533|0.61777|0.28703|   0.45901| 1.9757|
|             Belgium|      Western Europe|   19| 6.937|      0.03595|1.30782|1.28566|0.89667| 0.5845| 0.2254|    0.2225|2.41484|
|United Arab Emirates|Middle East and N...|   20| 6.901|      0.03729|1.42727|1.12575|0.80925|0.64157|0.38583|   0.26428|2.24743|
+--------------------+--------------------+-----+------+-------------+-------+-------+-------+-------+-------+----------+-------+
only showing top 20 rows

scala> val countfive = fivedf.count()
countfive: Long = 158 

scala> println(s"count of rows:$countfive")
count of rows:158

scala> val nullcounts = fivedf.select(fivedf.columns.map(c => sum(col(c).isNull.cast("int")).alias(c)):_*)
nullcounts: org.apache.spark.sql.DataFrame = [country: bigint, region: bigint ... 10 more fields]

scala> nullcounts.show()
+-------+------+-----+------+-------------+-------+------+------+-------+-----+----------+---+
|country|region|hrank|hscore|standarderror|economy|family|health|freedom|trust|generosity| dr|
+-------+------+-----+------+-------------+-------+------+------+-------+-----+----------+---+
|      0|     0|    0|     0|            0|      0|     0|     0|      0|    0|         0|  0|
+-------+------+-----+------+-------------+-------+------+------+-------+-----+----------+---+

scala> val deduplicatedDF = fivedf.dropDuplicates()
deduplicatedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country: string, region: string ... 10 more fields]

scala> val cleanedDF = deduplicatedDF.na.drop()
cleanedDF: org.apache.spark.sql.DataFrame = [country: string, region: string ... 10 more fields]

scala> val numRows = cleanedDF.count()
numRows: Long = 158                                                             

scala> fivedf.printSchema()
root
 |-- country: string (nullable = true)
 |-- region: string (nullable = true)
 |-- hrank: integer (nullable = true)
 |-- hscore: float (nullable = true)
 |-- standarderror: float (nullable = true)
 |-- economy: float (nullable = true)
 |-- family: float (nullable = true)
 |-- health: float (nullable = true)
 |-- freedom: float (nullable = true)
 |-- trust: float (nullable = true)
 |-- generosity: float (nullable = true)
 |-- dr: float (nullable = true)

scala> val processedData = cleanedDF.withColumnRenamed("dr","DystopiaResidual") .withColumnRenamed("hrank","HappinessRank") .withColumnRenamed("hscore","HappinessScore")
processedData: org.apache.spark.sql.DataFrame = [country: string, region: string ... 10 more fields]

scala> processedData.printSchema()
root
 |-- country: string (nullable = true)
 |-- region: string (nullable = true)
 |-- HappinessRank: integer (nullable = true)
 |-- HappinessScore: float (nullable = true)
 |-- standarderror: float (nullable = true)
 |-- economy: float (nullable = true)
 |-- family: float (nullable = true)
 |-- health: float (nullable = true)
 |-- freedom: float (nullable = true)
 |-- trust: float (nullable = true)
 |-- generosity: float (nullable = true)
 |-- DystopiaResidual: float (nullable = true)

processedData.write .mode("overwrite") .saveAsTable("caps_proj.ProcessedHWR2015")
