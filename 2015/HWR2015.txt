capstone project steps in ubuntu

HADOOP
	
	cd hadoop-2.9.1/
	sbin/start-all.sh
	jps

	bin/hadoop dfs -ls /
	bin/hadoop dfs -ls /capstone
	bin/hadoop dfs -cat /capstone/part-m-00000
	
MYSQL	sudo mysql -u root -p
	create database caps_proj;
	use caps_proj;
	create table HWR2015 (country varchar(50),region varchar(75),Hrank int,Hscore float,StandardError float,economy float,family float,health float,freedom float,trust float,generosity float,dr float);
	
	SHOW VARIABLES LIKE "secure_file_priv";
	/var/lib/mysql-files/
	
normal terminal 
	sudo cp /home/deivanai/Downloads/fwd/2015.csv /var/lib/mysql-files/
	sudo ls /var/lib/mysql-files/

Again MYSQL
	load data infile '/var/lib/mysql-files/2015.csv' into table HWR2015 fields terminated by ',' lines terminated by '\n' ignore 1 lines;
	
HIVE
	cd apache-hive-2.3.8-bin/
	bin/hive
	create database caps_proj;
	use caps_proj;
	create table HWR2015 (country string,region string,Hrank int,Hscore float,StandardError float,economy float,family float,health float,freedom float,trust float,generosity float,dr float) row format delimited ;

TO LOAD DATA FROM HDFS
	load data inpath '/capstone/part-m-00000' into table HWR2015;
	
MYSQL
	GRANT ALL PRIVILEGES ON caps_proj.* TO 'hiveuser'@'localhost';
	FLUSH PRIVILEGES;

	
SQOOP
	cd sqoop-1.4.7.bin__hadoop-2.6.0/
	MYSQL to HDFS
	bin/sqoop import --connect jdbc:mysql://localhost/caps_proj --username training --password training --table HWR2015 --target-dir /capstone -m1;
	MYSQL to HIVE
	bin/sqoop import --connect jdbc:mysql://localhost/caps_proj --username hiveuser --password password --table HWR2015 --hive-import --hive-table caps_proj.HWR2015 -m1;


SPARK  connecting hive and spark
	
	cd spark-2.4.6-bin-hadoop2.7/
	sbin/start-all.sh
	bin/spark-shell
	val fivedf = spark.sql("select * from caps_proj.HWR2015")
	fivedf.show()
