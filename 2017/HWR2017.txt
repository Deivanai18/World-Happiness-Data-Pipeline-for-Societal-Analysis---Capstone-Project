capstone project steps in ubuntu

HADOOP
	
	bin/hadoop dfs -ls /
	bin/hadoop dfs -ls /capstone2
	bin/hadoop dfs -cat /capstone2/part-m-00000
	
MYSQL	

	use caps_proj;
	create table HWR2017 (country varchar(50),Hrank int,Hscore float,whiskerHigh float,whiskerLow float,economy float,family float,health float,freedom float,generosity float,trust float,dr float);
	
	
normal terminal 
	sudo cp /home/deivanai/Downloads/fwd/2017.csv /var/lib/mysql-files/
	sudo ls /var/lib/mysql-files/

Again MYSQL
	load data infile '/var/lib/mysql-files/2017.csv' into table HWR2017 fields terminated by ',' lines terminated by '\n' ignore 1 lines;
	
HIVE

	use caps_proj;
	create table HWR2017 (country string,Hrank int,Hscore float,whiskerHigh float,whiskerLow float,economy float,family float,health float,freedom float,generosity float,trust float,dr float) row format delimited ;

TO LOAD DATA FROM HDFS
	load data inpath '/capstone2/part-m-00000' into table HWR2017;
	
	
SQOOP

	MYSQL to HDFS
	bin/sqoop import --connect jdbc:mysql://localhost/caps_proj --username training --password training --table HWR2017 --target-dir /capstone2 -m1;
	MYSQL to HIVE
	bin/sqoop import --connect jdbc:mysql://localhost/caps_proj --username hiveuser --password password --table HWR2017 --hive-import --hive-table caps_proj.HWR2017 -m1;


SPARK  connecting hive and spark
	
	val sevendf = spark.sql("select * from caps_proj.HWR2017")
	sevendf.show()
